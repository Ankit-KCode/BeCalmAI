MODEL_NAME = "meta-llama/Llama-2-7b-chat-hf"  # Choose a free LLaMA model

USE_CUDA = False  # Set to False if running on CPU
